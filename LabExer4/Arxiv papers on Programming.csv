"","title","author","subject","abstract","meta"
"1","Exploring Safety Generalization Challenges of Large Language Models via Code","Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma","Computation and Language (cs.CL)","The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to weaker safety generalization, such as encoding natural language input with data structures or using less popular programming languages. These findings highlight new safety risks in the code domain and the need for more robust safety alignment algorithms to match the code capabilities of LLMs.","Tue, 12 Mar 2024 17:55:38 UTC (8,284 KB)"
"2","Programming droplet motion using metamaterials","Mohammad Charara, Zak Kujala, Sungyon Lee, Stefano Gonella","Fluid Dynamics (physics.flu-dyn)","Motion control of droplets has generated much attention for its applications to microfluidics, where precise control of small fluid volumes is an imperative requirement. Mechanical vibrations have been shown to be effective at inducing controllable depinning, and activation of different drop motion regimes. However, existing vibration-based strategies involve establishing homogeneous rigid-body dynamics on the substrate, and therefore lack any form of spatial heterogeneity and tuning. Addressing this limitation, metamaterials provide an ideal platform to achieve spectrally and spatially selective drop motion control, which leverages their ability to attenuate vibrations in selected frequency bands and in selected regions of a substrate. In this work, we illustrate the potential of metamaterials-based drop control by experimentally demonstrating a variety of drop motion capabilities on the surface of metaplates endowed with locally resonant stubs. The experiments leverage the design versatility of a LEGO component-enabled reconfigurable design platform and laser vibrometry measurements with high spatial resolution.","Tue, 12 Mar 2024 17:25:30 UTC (26,411 KB)"
"3","Augmenting Interpolation-Based Model Checking with Auxiliary Invariants (Extended Version)","Dirk Beyer, Po-Chun Chien, Nian-Ze Lee","Software Engineering (cs.SE)","Software model checking is a challenging problem, and generating relevant invariants is a key factor in proving the safety properties of a program. Program invariants can be obtained by various approaches, including lightweight procedures based on data-flow analysis and intensive techniques using Craig interpolation. Although data-flow analysis runs efficiently, it often produces invariants that are too weak to prove the properties. By contrast, interpolation-based approaches build strong invariants from interpolants, but they might not scale well due to expensive interpolation procedures. Invariants can also be injected into model-checking algorithms to assist the analysis. Invariant injection has been studied for many well-known approaches, including k-induction, predicate abstraction, and symbolic execution. We propose an augmented interpolation-based verification algorithm that injects external invariants into interpolation-based model checking (McMillan, 2003), a hardware model-checking algorithm recently adopted for software verification. The auxiliary invariants help prune unreachable states in Craig interpolants and confine the analysis to the reachable parts of a program. We implemented the proposed technique in the verification framework CPAchecker and evaluated it against mature SMT-based methods in CPAchecker as well as other state-of-the-art software verifiers. We found that injecting invariants reduces the number of interpolation queries needed to prove safety properties and improves the run-time efficiency. Consequently, the proposed invariant-injection approach verified difficult tasks that none of its plain version (i.e., without invariants), the invariant generator, or any compared tools could solve.","Tue, 12 Mar 2024 17:02:53 UTC (976 KB)"
"4","PROSKILL: A formal skill language for acting in robotics","Félix Ingrand (LAAS-CNRS, Université de Toulouse, Toulouse, France)","Robotics (cs.RO)","Acting is an important decisional function for autonomous robots. Acting relies on skills to implement and to model the activities it oversees: refinement, local recovery, temporal dispatching, external asynchronous events, and commands execution, all done online. While sitting between planning and the robotic platform, acting often relies on programming primitives and an interpreter which executes these skills. Following our experience in providing a formal framework to program the functional components of our robots, we propose a new language, to program the acting skills. This language maps unequivocally into a formal model which can then be used to check properties offline or execute the skills, or more precisely their formal equivalent, and perform runtime verification. We illustrate with a real example how we can program a survey mission for a drone in this new language, prove some formal properties on the program and directly execute the formal model on the drone to perform the mission.","Tue, 12 Mar 2024 15:56:53 UTC (3,571 KB)"
"5","FAUST XI: Enhancement of the complex organic material in the shocked matter surrounding the [BHB2007] 11 protobinary system","C. Vastel, T. Sakai, C. Ceccarelli, I. Jiménez-Serra, F. Alves, N. Balucani, E. Bianchi, M. Bouvier, P. Caselli, C. J. Chandler, S. Charnley, C. Codella, M. De Simone, F. Dulieu, L. Evans, F. Fontani, B. Lefloch, L. Loinard, F. Menard, L. Podio, G. Sabatini, N. Sakai, S. Yamamoto","Solar and Stellar Astrophysics (astro-ph.SR)","iCOMs are species commonly found in the interstellar medium. They are believed to be crucial seed species for the build-up of chemical complexity in star forming regions as well as our own Solar System. Thus, understanding how their abundances evolve during the star formation process and whether it enriches the emerging planetary system is of paramount importance. We use data from the ALMA Large Program FAUST to study the compact line emission towards the [BHB2007] 11 proto-binary system (sources A and B), where a complex structure of filaments connecting the two sources with a larger circumbinary disk has previously been detected. More than 45 CH3OCHO lines are clearly detected, as well as 8 CH3OCH3 transitions , 1 H2CCO transition and 4 t-HCOOH transitions. We compute the abundance ratios with respect to CH3OH for CH3OCHO, CH3OCH3, H2CCO, t-HCOOH (as well as an upper limit for CH3CHO) through a radiative transfer analysis. We also report the upper limits on the column densities of nitrogen bearing iCOMs, N(C2H5CN) and N(C2H3CN). The emission from the detected iCOMs and their precursors is compact and encompasses both protostars, which are separated by only 0.2"" (~ 28 au). The integrated intensities tend to align with the Southern filament, revealed by the high spatial resolution observations of the dust emission at 1.3 mm. A PV and 2D analysis are performed on the strongest and uncontaminated CH3OCH3 transition and show three different spatial and velocity regions, two of them being close to 11B (Southern filament) and the third one near 11A. All our observations suggest that the detected methanol, as well as the other iCOMs, are generated by the shocked gas from the incoming filaments streaming towards [BHB2007] 11A and 11B, respectively, making this source one of the few where chemical enrichment of the gas caused by the streaming material is observed.","Tue, 12 Mar 2024 15:44:12 UTC (4,306 KB)"
"6","Tightening big Ms in integer programming formulations for support vector machines with ramp loss","Marta Baldomero-Naranjo, Luisa I. Martínez-Merino, Antonio M. Rodríguez-Chía","Optimization and Control (math.OC)","This paper considers various models of support vector machines with ramp loss, these being an efficient and robust tool in supervised classification for the detection of outliers. The exact solution approaches for the resulting optimization problem are of high demand for large datasets. Hence, the goal of this paper is to develop algorithms that provide efficient methodologies to exactly solve these optimization problems. These approaches are based on three strategies for obtaining tightened values of the big M parameters included in the formulation of the problem. Two of them require solving a sequence of continuous problems, while the third uses the Lagrangian relaxation to tighten the bounds. The proposed resolution methods are valid for the l1-norm and l2-norm ramp loss formulations. They were tested and compared with existing solution methods in simulated and real-life datasets, showing the efficiency of the developed methodology.","Tue, 12 Mar 2024 15:13:40 UTC (37 KB)"
"7","A Framework for Controlling Multiple Industrial Robots using Mobile Applications","Daniela Alvarado, Dr. Seemal Asif","Robotics (cs.RO)","Purpose: Over the last few decades, the development of the hardware and software has enabled the application of advanced systems. In the robotics field, the UI design is an intriguing area to be explored due to the creation of devices with a wide range of functionalities in a reduced size. Moreover, the idea of using the same UI to control several systems arouses a great interest considering that this involves less learning effort and time for the users. Therefore, this paper will present a mobile application to control two industrial robots with four modes of operation. Design/methodology/approach: The smartphone was selected to be the interface due to its wide range of capabilities and the MIT Inventor App was used to create the application, whose environment is supported by Android smartphones. For the validation, ROS was used since it is a fundamental framework utilised in industrial robotics and the Arduino Uno was used to establish the data transmission between the smartphone and the board NVIDIA Jetson TX2. In MIT Inventor App, the graphical interface was created to visualize the options available in the app whereas two scripts in python were programmed to perform the simulations in ROS and carry out the tests. Findings: The results indicated that the use of the sliders to control the robots is more favourable than the Orientation Sensor due to the sensibility of the sensor and human limitations to hold the smartphone perfectly still. Another important finding was the limitations of the autonomous mode, in which the robot grabs an object. In this case, the configuration of the Kinect camera and the controllers has a significant impact on the success of the simulation. Finally, it was observed that the delay was appropriate despite the use of the Arduino UNO to transfer the data between the Smartphone and the Nvidia Jetson TX2.","Tue, 12 Mar 2024 13:23:40 UTC (940 KB)"
"8","Couler: Unified Machine Learning Workflow Optimization in Cloud","Xiaoda Wang, Yuan Tang, Tengda Guo, Bo Sang, Jingji Wu, Jian Sha, Ke Zhang, Jiang Qian, Mingjie Tang","Databases (cs.DB)","Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines. In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions. We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines. This approach alleviates the need to understand various workflow engines' APIs. Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning. These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%.","Tue, 12 Mar 2024 12:47:32 UTC (1,012 KB)"
"9","On Weakly Contracting Dynamics for Convex Optimization","Veronica Centorrino, Alexander Davydov, Anand Gokhale, Giovanni Russo, Francesco Bullo","Optimization and Control (math.OC)","We investigate the convergence characteristics of dynamics that are \emph{globally weakly} and \emph{locally strongly contracting}. Such dynamics naturally arise in the context of convex optimization problems with a unique minimizer. We show that convergence to the equilibrium is \emph{linear-exponential}, in the sense that the distance between each solution and the equilibrium is upper bounded by a function that first decreases linearly and then exponentially. As we show, the linear-exponential dependency arises naturally in certain dynamics with saturations. Additionally, we provide a sufficient condition for local input-to-state stability. Finally, we illustrate our results on, and propose a conjecture for, continuous-time dynamical systems solving linear programs.","Tue, 12 Mar 2024 12:02:48 UTC (1,773 KB)"
"10","Energy versus Output Quality of Non-volatile Writes in Intermittent Computing","Rei Barjami, Antonio Miele, Luca Mottola","Systems and Control (eess.SY)","We explore how to improve the energy performance of battery-less Internet of Things (IoT) devices at the cost of a reduction in the quality of the output. Battery-less IoT devices are extremely resource-constrained energy-harvesting devices. Due to erratic energy patterns from the ambient, their executions become intermittent; periods of active computation are interleaved by periods of recharging small energy buffers. To cross periods of energy unavailability, a device persists application and system state onto Non-Volatile Memory (NVM) in anticipation of energy failures. We purposely control the energy invested in these operations, representing a major energy overhead, when using Spin-Transfer Torque Magnetic Random-Access Memory (STT-MRAM) as NVM. As a result, we abate the corresponding overhead, yet introduce write errors. Based on 1.9+ trillion experimental data points, we illustrate whether this is a gamble worth taking, when, and where. We measure the energy consumption and quality of output obtained from the execution of nine diverse benchmarks on top of seven different platforms. Our results allow us to draw three key observations: i) the trade-off between energy saving and reduction of output quality is program-specific; ii) the same trade-off is a function of a platform's specific compute efficiency and power figures; and iii) data encoding and input size impact a program's resilience to errors. As a paradigmatic example, we reveal cases where we achieve up to 50% reduction in energy consumption with negligible effects on output quality, as opposed to settings where a minimal energy gain causes drastic drops in output quality.","Tue, 12 Mar 2024 10:50:13 UTC (10,295 KB)"
"11","DrPlanner: Diagnosis and Repair of Motion Planners Using Large Language Models","Yuanfei Lin, Chenran Li, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan, Matthias Althoff","Robotics (cs.RO)","Motion planners are essential for the safe operation of automated vehicles across various scenarios. However, no motion planning algorithm has achieved perfection in the literature, and improving its performance is often time-consuming and labor-intensive. To tackle the aforementioned issues, we present DrPlanner, the first framework designed to automatically diagnose and repair motion planners using large language models. Initially, we generate a structured description of the planner and its planned trajectories from both natural and programming languages. Leveraging the profound capabilities of large language models in addressing reasoning challenges, our framework returns repaired planners with detailed diagnostic descriptions. Furthermore, the framework advances iteratively with continuous feedback from the evaluation of the repaired outcomes. Our approach is validated using search-based motion planners; experimental results highlight the need of demonstrations in the prompt and the ability of our framework in identifying and rectifying elusive issues effectively.","Tue, 12 Mar 2024 10:06:17 UTC (2,709 KB)"
"12","One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices","Marco Chilese, Richard Mitev, Meni Orenbach, Robert Thorburn, Ahmad Atamli, Ahmad-Reza Sadeghi","Cryptography and Security (cs.CR)","Control-Flow Attestation (CFA) is a security service that allows an entity (verifier) to verify the integrity of code execution on a remote computer system (prover). Existing CFA schemes suffer from impractical assumptions, such as requiring access to the prover's internal state (e.g., memory or code), the complete Control-Flow Graph (CFG) of the prover's software, large sets of measurements, or tailor-made hardware. Moreover, current CFA schemes are inadequate for attesting embedded systems due to their high computational overhead and resource usage. In this paper, we overcome the limitations of existing CFA schemes for embedded devices by introducing RAGE, a novel, lightweight CFA approach with minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including control- and non-control-data attacks. It efficiently extracts features from one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to identify deviations from benign executions. The core intuition behind RAGE is to exploit the correspondence between execution trace, execution graph, and execution embeddings to eliminate the unrealistic requirement of having access to a complete CFG. We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects 40 real-world attacks on embedded software; (ii) Further, we stress our scheme with synthetic return-oriented programming (ROP) and data-oriented programming (DOP) attacks on the real-world embedded software benchmark Embench, achieving 98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP attack detection, with an FPR of 5.47%.","Tue, 12 Mar 2024 10:00:06 UTC (1,447 KB)"
"13","Fixing Smart Contract Vulnerabilities: A Comparative Analysis of Literature and Developer's Practices","Francesco Salzano, Simone Scalabrino, Rocco Oliveto, Remo Pareschi","Software Engineering (cs.SE)","Smart Contracts are programs running logic in the Blockchain network by executing operations through immutable transactions. The Blockchain network validates such transactions, storing them into sequential blocks of which integrity is ensured. Smart Contracts deal with value stakes, if a damaging transaction is validated, it may never be reverted, leading to unrecoverable losses. To prevent this, security aspects have been explored in several fields, with research providing catalogs of security defects, secure code recommendations, and possible solutions to fix vulnerabilities. In our study, we refer to vulnerability fixing in the ways found in the literature as guidelines. However, it is not clear to what extent developers adhere to these guidelines, nor whether there are other viable common solutions and what they are. The goal of our research is to fill knowledge gaps related to developers' observance of existing guidelines and to propose new and viable solutions to security vulnerabilities. To reach our goal, we will obtain from Solidity GitHub repositories the commits that fix vulnerabilities included in the DASP TOP 10 and we will conduct a manual analysis of fixing approaches employed by developers. Our analysis aims to determine the extent to which literature-based fixing strategies are followed. Additionally, we will identify and discuss emerging fixing techniques not currently documented in the literature. Through qualitative analysis, we will evaluate the suitability of these new fixing solutions and discriminate between valid approaches and potential mistakes.","Tue, 12 Mar 2024 09:55:54 UTC (968 KB)"
"14","Energy bounds for weighted spherical codes and designs via linear programming","Sergiy Borodachov, Peter Boyvalenkov, Peter Dragnev, Douglas Hardin, Edward Saff, Maya Stoyanova","Metric Geometry (math.MG)","Universal bounds for the potential energy of weighted spherical codes are obtained by linear programming. The universality is in the sense of Cohn-Kumar -- every attaining code is optimal with respect to a large class of potential functions (absolutely monotone), in the sense of Levenshtein -- there is a bound for every weighted code, and in the sense of parameters (nodes and weights) -- they are independent of the potential function. We derive a necessary condition for optimality (in the linear programming framework) of our lower bounds which is also shown to be sufficient when the potential is strictly absolutely monotone. Bounds are also obtained for the weighted energy of weighted spherical designs. We explore our bounds for several previously studied weighted spherical codes.","Tue, 12 Mar 2024 09:55:06 UTC (28 KB)"
"15","Atomicity and Abstraction for Cross-Blockchain Interactions","Huaixi Lu, Akshay Jajoo, Kedar S. Namjoshi","Cryptography and Security (cs.CR)","A blockchain facilitates secure and atomic transactions between mutually untrusting parties on that chain. Today, there are multiple blockchains with differing interfaces and security properties. Programming in this multi-blockchain world is hindered by the lack of general and convenient abstractions for cross-chain communication and computation. Current cross-chain communication bridges have varied and low-level interfaces, making it difficult to develop portable applications. Current methods for multi-chain atomic transactions are limited in scope to cryptocurrency swaps. This work addresses these issues. We first define a uniform, high-level interface for communication between chains. Building on this interface, we formulate a protocol that guarantees atomicity for general transactions whose operations may span several chains. We formulate and prove the desired correctness and security properties of these protocols. Our prototype implementation is built using the LayerZero cross-chain bridge. Experience with this implementation shows that the new abstractions considerably simplify the design and implementation of multi-chain transactions. Experimental evaluation with multi-chain swap transactions demonstrates performance comparable to that of custom-built implementations.","Tue, 12 Mar 2024 02:13:29 UTC (905 KB)"
"16","The Primal Pathwidth SETH","Michael Lampis","Computational Complexity (cs.CC)","Motivated by the importance of dynamic programming (DP) in parameterized complexity, we consider several fine-grained questions, such as the following examples: (i) can Dominating Set be solved in time $(3-\epsilon)^{pw}n^{O(1)}$? (where $pw$ is the pathwidth) (ii) can Coloring be solved in time $pw^{(1-\epsilon)pw}n^{O(1)}$? (iii) can a short reconfiguration between two size-$k$ independent sets be found in time $n^{(1-\epsilon)k}$? Such questions are well-studied: in some cases the answer is No under the SETH, while in others coarse-grained lower bounds are known under the ETH. Even though questions such as the above seem ""morally equivalent"" as they all ask if a simple DP can be improved, the problems concerned have wildly varying time complexities, ranging from single-exponential FPT to XNLP-complete. This paper's main contribution is to show that, despite their varying complexities, these questions are not just morally equivalent, but in fact they are the same question in disguise. We achieve this by putting forth a natural complexity assumption which we call the Primal Pathwidth-Strong Exponential Time Hypothesis (PP-SETH) and which states that 3-SAT cannot be solved in time $(2-\epsilon)^{pw}n^{O(1)}$, for any $\epsilon>0$, where $pw$ is the pathwidth of the primal graph of the input. We then show that numerous fine-grained questions in parameterized complexity, including the ones above, are equivalent to the PP-SETH, and hence to each other. This allows us to obtain sharp fine-grained lower bounds for problems for which previous lower bounds left a constant in the exponent undetermined, but also to increase our confidence in bounds which were previously known under the SETH, because we show that breaking any one such bound requires breaking all (old and new) bounds; and because we show that the PP-SETH is more plausible than the SETH.","Tue, 12 Mar 2024 01:21:22 UTC (299 KB)"
"17","Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model","Sarah Moon","Econometrics (econ.EM)","It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships, but only has access to data that has been aggregated. In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary, while imposing as few restrictions on the underlying data generating process as possible. I construct identified sets using an optimization program that allows for researchers to impose additional shape restrictions. I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful. This suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified.","Tue, 12 Mar 2024 01:14:35 UTC (142 KB)"
"18","Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding Window Approach","Yinsong Wang, Yu Ding, Shahin Shahrampour","Machine Learning (stat.ML)","Dynamic density estimation is ubiquitous in many applications, including computer vision and signal processing. One popular method to tackle this problem is the ""sliding window"" kernel density estimator. There exist various implementations of this method that use heuristically defined weight sequences for the observed data. The weight sequence, however, is a key aspect of the estimator affecting the tracking performance significantly. In this work, we study the exact mean integrated squared error (MISE) of ""sliding window"" Gaussian Kernel Density Estimators for evolving Gaussian densities. We provide a principled guide for choosing the optimal weight sequence by theoretically characterizing the exact MISE, which can be formulated as constrained quadratic programming. We present empirical evidence with synthetic datasets to show that our weighting scheme indeed improves the tracking performance compared to heuristic approaches.","Mon, 11 Mar 2024 23:21:26 UTC (400 KB)"
"19","New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production","Shiliang Zuo","Computer Science and Game Theory (cs.GT)","This work studies the repeated principal-agent problem from an online learning perspective. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions). I study three different settings when the principal contracts with a $\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic. I present different approaches and techniques for designing learning algorithms in each setting. For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz bandits directly. For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory. For strategic non-myopic agents, I design a low strategic-regret mechanism. Also, I identify a connection between linear contracts and posted-price auctions, showing the two can be reduced to one another, and give a regret lower bound on learning the optimal linear contract based on this observation. I also study a $\textit{team production}$ model. I identify a condition under which the principal's learning problem can be reformulated as solving a family of convex programs, thereby showing the optimal contract can be found efficiently.","Mon, 11 Mar 2024 20:28:23 UTC (63 KB)"
"20","Advanced-Step Real-time Iterations with Four Levels -- New Error Bounds and Fast Implementation in acados","Jonathan Frey, Armin Nurkanovic, Moritz Diehl","Optimization and Control (math.OC)","The Real-Time Iteration (RTI) is an online nonlinear model predictive control algorithm that performs a single Sequential Quadratic Programming (SQP) per sampling time. The algorithm is split into a preparation and a feedback phase, where the latter one performs as little computations as possible solving a single prepared quadratic program. To further improve the accuracy of this method, the Advanced-Step RTI (AS-RTI) performs additional Multi-Level Iterations (MLI) in the preparation phase, such as inexact or zero-order SQP iterations on a problem with a predicted state estimate. This paper extends and streamlines the existing analysis of AS-RTI, such as analyzing MLI of level A and B for the first time, and significantly simplifying the proofs for levels C and D. Moreover, this paper provides an efficient open-source implementation in acados, making it widely accessible to practitioners.","Mon, 11 Mar 2024 18:52:46 UTC (528 KB)"
"21","FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning","Xiang Meng, Wenyu Chen, Riade Benbaki, Rahul Mazumder","Machine Learning (cs.LG)","The increasing computational demands of modern neural networks present deployment challenges on resource-constrained devices. Network pruning offers a solution to reduce model size and computational cost while maintaining performance. However, most current pruning methods focus primarily on improving sparsity by reducing the number of nonzero parameters, often neglecting other deployment costs such as inference time, which are closely related to the number of floating-point operations (FLOPs). In this paper, we propose FALCON, a novel combinatorial-optimization-based framework for network pruning that jointly takes into account model accuracy (fidelity), FLOPs, and sparsity constraints. A main building block of our approach is an integer linear program (ILP) that simultaneously handles FLOP and sparsity constraints. We present a novel algorithm to approximately solve the ILP. We propose a novel first-order method for our optimization framework which makes use of our ILP solver. Using problem structure (e.g., the low-rank structure of approx. Hessian), we can address instances with millions of parameters. Our experiments demonstrate that FALCON achieves superior accuracy compared to other pruning approaches within a fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs retained, our approach improves the accuracy by 48% relative to state-of-the-art. Furthermore, in gradual pruning settings with re-training between pruning steps, our framework outperforms existing pruning methods, emphasizing the significance of incorporating both FLOP and sparsity constraints for effective network pruning.","Mon, 11 Mar 2024 18:40:47 UTC (1,042 KB)"
"22","HST Survey of the Orion Nebula Cluster in ACS/Visible and WFC3/IR Bands. IV. A Bayesian multicolor study of stellar parameters in the ONC","Giovanni M. Strampelli, Massimo Robberto, Laurent Pueyo, Mario Gennaro, Carlo F. Manara, Elena Sabbi, Antonio Aparicio","Solar and Stellar Astrophysics (astro-ph.SR)","We have performed a comprehensive study of the Orion Nebula Cluster (ONC) combining the photometric data obtained by the two \textit{HST} Treasury programs that targeted this region. To consistently analyze the rich dataset obtained in a wide variety of filters, we adopted a Bayesian approach to fit the Spectral Energy Distribution of the sources, deriving mass, age, extinction, distance, and accretion for each source in the region. The three dimensional study of mass distribution for bona-fide cluster members shows that mass segregation in the ONC extends to sub-solar masses, while the age distribution strongly supports the idea that star formation in the ONC is best described by a major episode of star formation that happened $\sim 1$ Myr ago. For masses $\gtrsim 0.1$ \Msun, our derived empirical initial mass function (IMF) is in good agreement with a Chabrier system IMF. Both the accretion luminosity (\Lacc) and mass accretion rates (\dMacc) are best described by broken power-law relations. This suggests that for the majority of young circumstellar disks in this cluster the excess emission may be dominated by X-ray-driven photoevaporation by the central star rather than external photoevaporation. If this is the case, the slopes of the power-law relations may be largely determined by the initial conditions set at the onset of the star formation process, which may be quite similar between regions that eventually form clusters of different sizes.","Mon, 11 Mar 2024 18:18:19 UTC (4,212 KB)"
"23","A Collision Cone Approach for Control Barrier Functions","Manan Tayal, Bhavya Giri Goswami, Karthik Rajgopal, Rajpal Singh, Tejas Rao, Jishnu Keshavan, Pushpak Jagtap, Shishir Kolathaya","Robotics (cs.RO)","This work presents a unified approach for collision avoidance using Collision-Cone Control Barrier Functions (CBFs) in both ground (UGV) and aerial (UAV) unmanned vehicles. We propose a novel CBF formulation inspired by collision cones, to ensure safety by constraining the relative velocity between the vehicle and the obstacle to always point away from each other. The efficacy of this approach is demonstrated through simulations and hardware implementations on the TurtleBot, Stoch-Jeep, and Crazyflie 2.1 quadrotor robot, showcasing its effectiveness in avoiding collisions with dynamic obstacles in both ground and aerial settings. The real-time controller is developed using CBF Quadratic Programs (CBF-QPs). Comparative analysis with the state-of-the-art CBFs highlights the less conservative nature of the proposed approach. Overall, this research contributes to a novel control formation that can give a guarantee for collision avoidance in unmanned vehicles by modifying the control inputs from existing path-planning controllers.","Mon, 11 Mar 2024 17:39:16 UTC (20,375 KB)"
"24","From English to ASIC: Hardware Implementation with Large Language Model","Emil Goh, Maoyang Xiang, I-Chyn Wey, T. Hui Teo","Hardware Architecture (cs.AR)","In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of LLM, paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. To address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The fine-tuning aims to enhance models' proficiency in generating precise and efficient ASIC design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. The model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. This approach is expected to facilitate a simplified and more efficient LLM-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of HDL coding and thus streamlining the ASIC development process.","Mon, 11 Mar 2024 09:57:16 UTC (669 KB)"
"25","A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary Modifiable Areal Units","Liyue Chen, Jiangyi Fang, Tengfei Liu, Shaosheng Cao, Leye Wang","Machine Learning (cs.LG)","Spatio-Temporal (ST) prediction is crucial for making informed decisions in urban location-based applications like ride-sharing. However, existing ST models often require region partition as a prerequisite, resulting in two main pitfalls. Firstly, location-based services necessitate ad-hoc regions for various purposes, requiring multiple ST models with varying scales and zones, which can be costly to support. Secondly, different ST models may produce conflicting outputs, resulting in confusing predictions. In this paper, we propose One4All-ST, a framework that can conduct ST prediction for arbitrary modifiable areal units using only one model. To reduce the cost of getting multi-scale predictions, we design an ST network with hierarchical spatial modeling and scale normalization modules to efficiently and equally learn multi-scale representations. To address prediction inconsistencies across scales, we propose a dynamic programming scheme to solve the formulated optimal combination problem, minimizing predicted error through theoretical analysis. Besides, we suggest using an extended quad-tree to index the optimal combinations for quick response to arbitrary modifiable areal units in practical online scenarios. Extensive experiments on two real-world datasets verify the efficiency and effectiveness of One4All-ST in ST prediction for arbitrary modifiable areal units. The source codes and data of this work are available at this https URL.","Sun, 10 Mar 2024 02:34:44 UTC (11,707 KB)"
"26","Solving Functional Equations Dear to W.T. Tutte using the Naive (yet fullly rigorous!) Guess And Check Method","Shalosh B. Ekhad, Doron Zeilberger","Combinatorics (math.CO)","In his seminal paper ``A census of planar triangulations"", published in 1962, the iconic graph theorist (and code-breaker), W.T. Tutte, spent a few pages to prove that a certain bi-variate generating function that enumerates triangulations, satisfies a certain functional equation. He then used his genius to actually solve it, giving closed-form solutions to the enumerating sequences. While the first part, of deriving the functional equation, still needs human ingenuity, the second part, of solving it, can nowadays be fully automated. Our Maple program, accompanying this paper, Tutte.txt, can not only solve Tutte's original equation in a few seconds, it can also solve many, far more complicated ones, way beyond the scope of even such a giant as W.T. Tutte. We use our favorite method of ``guess and check"" and show how it can always be made fully rigorous (if desired).","Sat, 9 Mar 2024 03:00:25 UTC (4 KB)"
"27","Dynamic Frequency Assignment for Mobile Users in Multibeam Satellite Constellations","Guillem Casadesus-Vila, Juan Jose Garau-Luis, Nils Pachler, Edward Crawley, Bruce Cameron","Networking and Internet Architecture (cs.NI)","Mobile users such as airplanes or ships will constitute an important segment of the future satellite communications market. Operators are now able to leverage digital payloads that allow flexible resource allocation policies that are robust against dynamic user bases. One of the key problems is managing the frequency spectrum efficiently, which has not been sufficiently explored for mobile users. To address this gap, we propose a dynamic frequency management algorithm based on linear programming that assigns resources in scenarios with both fixed and mobile users by combining long-term planning with real-time operation. We propose different strategies divided into proactive strategies, which stem from robust optimization practices, and reactive strategies, which exploit a high degree of real-time control. This represents a tradeoff between how conservative long-time planning should be and how much real-time reconfiguration is needed. To assess the performance of our method and to determine which proactive and reactive strategies work better under which context, we simulate operational use cases of non-geostationary constellations with different levels of dimensionality and uncertainty, showing that our method is able to serve over 99.97\% of the fixed and mobile users in scenarios with more than 900 beams. Finally, we discuss the trade-offs between the studied strategies in terms of the number of served users, power consumption, and number of changes that need to happen during operations.","Sat, 9 Mar 2024 01:45:47 UTC (28,225 KB)"
"28","Convergence of Some Convex Message Passing Algorithms to a Fixed Point","Vaclav Voracek, Tomas Werner","Artificial Intelligence (cs.AI)","A popular approach to the MAP inference problem in graphical models is to minimize an upper bound obtained from a dual linear programming or Lagrangian relaxation by (block-)coordinate descent. Examples of such algorithms are max-sum diffusion and sequential tree-reweighted message passing. Convergence properties of these methods are currently not fully understood. They have been proved to converge to the set characterized by local consistency of active constraints, with unknown convergence rate; however, it was not clear if the iterates converge at all (to any single point). We prove a stronger result (which was conjectured before but never proved): the iterates converge to a fixed point of the algorithm. Moreover, we show that they achieve precision $\varepsilon>0$ in $\mathcal{O}(1/\varepsilon)$ iterations. We first prove this for a version of coordinate descent applied to a general piecewise-affine convex objective, using a novel proof technique. Then we demonstrate the generality of this approach by reducing some popular coordinate-descent algorithms to this problem. Finally we show that, in contrast to our main result, a similar version of coordinate descent applied to a constrained optimization problem need not converge.","Thu, 7 Mar 2024 13:14:21 UTC (129 KB)"
"29","Exact algorithms and heuristics for capacitated covering salesman problems","Lucas Porto Maziero, Fábio Luiz Usberti, Celso Cavellucci","Artificial Intelligence (cs.AI)","This paper introduces the Capacitated Covering Salesman Problem (CCSP), approaching the notion of service by coverage in capacitated vehicle routing problems. In CCSP, locations where vehicles can transit are provided, some of which have customers with demands. The objective is to service customers through a fleet of vehicles based in a depot, minimizing the total distance traversed by the vehicles. CCSP is unique in the sense that customers, to be serviced, do not need to be visited by a vehicle. Instead, they can be serviced if they are within a coverage area of the vehicle. This assumption is motivated by applications in which some customers are unreachable (e.g., forbidden access to vehicles) or visiting every customer is impractical. In this work, optimization methodologies are proposed for the CCSP based on ILP (Integer Linear Programming) and BRKGA (Biased Random-Key Genetic Algorithm) metaheuristic. Computational experiments conducted on a benchmark of instances for the CCSP evaluate the performance of the methodologies with respect to primal bounds. Furthermore, our ILP formulation is extended in order to create a novel MILP (Mixed Integer Linear Programming) for the Multi-Depot Covering Tour Vehicle Routing Problem (MDCTVRP). Computational experiments show that the extended MILP formulation outperformed the previous state-of-the-art exact approach with respect to optimality gaps. In particular, optimal solutions were obtained for several previously unsolved instances.","Sun, 3 Mar 2024 07:50:29 UTC (738 KB)"
"30","Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints","Jean V. Alves, Diogo Leitão, Sérgio Jesus, Marco O. P. Sampaio, Javier Liébana, Pedro Saleiro, Mário A. T. Figueiredo, Pedro Bizarro","Machine Learning (cs.LG)","Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints. The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost.","Mon, 11 Mar 2024 16:57:20 UTC (450 KB)"
"31","Genetic Learning for Designing Sim-to-Real Data Augmentations","Bram Vanherle, Nick Michiels, Frank Van Reeth","Computer Vision and Pattern Recognition (cs.CV)","Data augmentations are useful in closing the sim-to-real domain gap when training on synthetic data. This is because they widen the training data distribution, thus encouraging the model to generalize better to other domains. Many image augmentation techniques exist, parametrized by different settings, such as strength and probability. This leads to a large space of different possible augmentation policies. Some policies work better than others for overcoming the sim-to-real gap for specific datasets, and it is unclear why. This paper presents two different interpretable metrics that can be combined to predict how well a certain augmentation policy will work for a specific sim-to-real setting, focusing on object detection. We validate our metrics by training many models with different augmentation policies and showing a strong correlation with performance on real data. Additionally, we introduce GeneticAugment, a genetic programming method that can leverage these metrics to automatically design an augmentation policy for a specific dataset without needing to train a model.","Mon, 11 Mar 2024 15:00:56 UTC (130 KB)"
"32","Domain-Independent Dynamic Programming and Constraint Programming Approaches for Assembly Line Balancing Problems with Setups","Jiachen Zhang, J. Christopher Beck","Optimization and Control (math.OC)","We propose domain-independent dynamic programming (DIDP) and constraint programming (CP) models to exactly solve type-1 and type-2 assembly line balancing problem with sequence-dependent setup times (SUALBP). The goal is to assign tasks to assembly stations and to sequence these tasks within each station, while satisfying precedence relations specified between a subset of task pairs. Each task has a given processing time and a setup time dependent on the previous task on the station to which the task is assigned. The sum of the processing and setup times of tasks assigned to each station constitute the station time and the maximum station time is called the cycle time. For type-1 SUALBP, the objective is to minimize the number of stations, given a maximum cycle time. For type-2 SUALBP, the objective is to minimize the cycle time, given the number of stations. On a set of diverse SUALBP instances, experimental results show that our approaches significantly outperform the state-of-the-art mixed integer programming models for SUALBP-1. For SUALBP-2, the DIDP model outperforms the state-of-the-art exact approach based on logic-based Benders decomposition. By closing 76 open instances for SUALBP-2, our results demonstrate the promise of DIDP for solving complex planning and scheduling problems.","Mon, 11 Mar 2024 14:51:34 UTC (281 KB)"
"33","Born to Run, Programmed to Play: Mapping the Extended Reality Exergames Landscape","Sukran Karaosmanoglu, Sebastian Cmentowski, Lennart E. Nacke, Frank Steinicke","Human-Computer Interaction (cs.HC)","Many people struggle to exercise regularly, raising the risk of serious health-related issues. Extended reality (XR) exergames address these hurdles by combining physical exercises with enjoyable, immersive gameplay. While a growing body of research explores XR exergames, no previous review has structured this rapidly expanding research landscape. We conducted a scoping review of the current state of XR exergame research to (i) provide a structured overview, (ii) highlight trends, and (iii) uncover knowledge gaps. After identifying 1318 papers in human-computer interaction and medical databases, we ultimately included 186 papers in our analysis. We provide a quantitative and qualitative summary of XR exergame research, showing current trends and potential future considerations. Finally, we provide a taxonomy of XR exergames to help future design and methodological investigation and reporting.","Mon, 11 Mar 2024 14:43:56 UTC (1,793 KB)"
"34","FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation","Pengchong Qiao, Lei Shang, Chang Liu, Baigui Sun, Xiangyang Ji, Jie Chen","Computer Vision and Pattern Recognition (cs.CV)","Subject-driven generation has garnered significant interest recently due to its ability to personalize text-to-image generation. Typical works focus on learning the new subject's private attributes. However, an important fact has not been taken seriously that a subject is not an isolated new concept but should be a specialization of a certain category in the pre-trained model. This results in the subject failing to comprehensively inherit the attributes in its category, causing poor attribute-related generations. In this paper, motivated by object-oriented programming, we model the subject as a derived class whose base class is its semantic category. This modeling enables the subject to inherit public attributes from its category while learning its private attributes from the user-provided example. Specifically, we propose a plug-and-play method, Subject-Derived regularization (SuDe). It constructs the base-derived class modeling by constraining the subject-driven generated images to semantically belong to the subject's category. Extensive experiments under three baselines and two backbones on various subjects show that our SuDe enables imaginative attribute-related generations while maintaining subject fidelity. Codes will be open sourced soon at FaceChain (this https URL).","Mon, 11 Mar 2024 14:43:40 UTC (20,817 KB)"
"35","SMC-Last Extracted Photometry","T. A. Kuchar, G. C. Sloan, D. R. Mizuno, Kathleen E. Kraemer, M. L. Boyer, Martin A. T. Groenewegen, O. C. Jones, F. Kemper, Iain McDonald, Joana M. Oliveira, Marta Sewiło, Sundar Srinivasan, Jacco Th. van Loon, Albert Zijlstra","Astrophysics of Galaxies (astro-ph.GA)","We present point-source photometry from the Spitzer Space Telescope's final survey of the Small Magellanic Cloud (SMC). We mapped 30 square degrees in two epochs in 2017, with the second extending to early 2018 at 3.6 and 4.5 microns using the Infrared Array Camera. This survey duplicates the footprint from the SAGE-SMC program in 2008. Together, these surveys cover a nearly 10 yr temporal baseline in the SMC. We performed aperture photometry on the mosaicked maps produced from the new data. We did not use any prior catalogs as inputs for the extractor in order to be sensitive to any moving objects (e.g., foreground brown dwarfs) and other transient phenomena (e.g., cataclysmic variables or FU Ori-type eruptions). We produced a point-source catalog with high-confidence sources for each epoch as well as combined-epoch catalog. For each epoch and the combined-epoch data, we also produced a more complete archive with lower-confidence sources. All of these data products will be available to the community at the Infrared Science Archive.","Mon, 11 Mar 2024 14:29:12 UTC (1,596 KB)"
"36","Galaxy Morphologies Revealed with Subaru HSC and Super-Resolution Techniques II: Environmental Dependence of Galaxy Mergers at z~2-5","Takatoshi Shibuya, Yohito Ito, Kenta Asai, Takanobu Kirihara, Seiji Fujimoto, Yoshiki Toba, Noriaki Miura, Takuya Umayahara, Kenji Iwadate, Sadman S. Ali, Tadayuki Kodama","Astrophysics of Galaxies (astro-ph.GA)","We super-resolve the seeing-limited Subaru Hyper Suprime-Cam (HSC) images for 32,187 galaxies at z~2-5 in three techniques, namely, the classical Richardson-Lucy (RL) point spread function (PSF) deconvolution, sparse modeling, and generative adversarial networks to investigate the environmental dependence of galaxy mergers. These three techniques generate overall similar high spatial resolution images but with some slight differences in galaxy structures, for example, more residual noises are seen in the classical RL PSF deconvolution. To alleviate disadvantages of each technique, we create combined images by averaging over the three types of super-resolution images, which result in galaxy sub-structures resembling those seen in the Hubble Space Telescope images. Using the combined super-resolution images, we measure the relative galaxy major merger fraction corrected for the chance projection effect, f_merg, for galaxies in the ~300 deg^2-area data of the HSC Strategic Survey Program and the CFHT Large Area U-band Survey. Our f_merg measurements at z~3 validate previous findings showing that f_merg is higher in regions with a higher galaxy overdensity delta at z~2-3. Thanks to the large galaxy sample, we identify a nearly linear increase in f_merg with increasing delta at z~4-5, providing the highest-z observational evidence that galaxy mergers are related to delta. In addition to our f_merg measurements, we find that the galaxy merger fractions in the literature also broadly align with the linear f_merg-delta relation across a wide redshift range of z~2-5. This alignment suggests that the linear f_merg-delta relation can serve as a valuable tool for quantitatively estimating the contributions of galaxy mergers to various environmental dependences. This super-resolution analysis can be readily applied to datasets from wide field-of-view space telescopes such as Euclid and Roman.","Mon, 11 Mar 2024 13:47:51 UTC (1,907 KB)"
"37","The Ouroboros of Memristors: Neural Networks Facilitating Memristor Programming","Zhenming Yu, Ming-Jay Yang, Jan Finkbeiner, Sebastian Siegel, John Paul Strachan, Emre Neftci","Emerging Technologies (cs.ET)","Memristive devices hold promise to improve the scale and efficiency of machine learning and neuromorphic hardware, thanks to their compact size, low power consumption, and the ability to perform matrix multiplications in constant time. However, on-chip training with memristor arrays still faces challenges, including device-to-device and cycle-to-cycle variations, switching non-linearity, and especially SET and RESET asymmetry. To combat device non-linearity and asymmetry, we propose to program memristors by harnessing neural networks that map desired conductance updates to the required pulse times. With our method, approximately 95% of devices can be programmed within a relative percentage difference of +-50% from the target conductance after just one attempt. Our approach substantially reduces memristor programming delays compared to traditional write-and-verify methods, presenting an advantageous solution for on-chip training scenarios. Furthermore, our proposed neural network can be accelerated by memristor arrays upon deployment, providing assistance while reducing hardware overhead compared with previous works. This work contributes significantly to the practical application of memristors, particularly in reducing delays in memristor programming. It also envisions the future development of memristor-based machine learning accelerators.","Mon, 11 Mar 2024 13:37:18 UTC (28,975 KB)"
"38","Deriving Dependently-Typed OOP from First Principles -- Extended Version with Additional Appendices","David Binder, Ingo Skupin, Tim Süberkrüb, Klaus Ostermann","Programming Languages (cs.PL)","The expression problem describes how most types can easily be extended with new ways to produce the type or new ways to consume the type, but not both. When abstract syntax trees are defined as an algebraic data type, for example, they can easily be extended with new consumers, such as print or eval, but adding a new constructor requires the modification of all existing pattern matches. The expression problem is one way to elucidate the difference between functional or data-oriented programs (easily extendable by new consumers) and object-oriented programs (easily extendable by new producers). This difference between programs which are extensible by new producers or new consumers also exists for dependently typed programming, but with one core difference: Dependently-typed programming almost exclusively follows the functional programming model and not the object-oriented model, which leaves an interesting space in the programming language landscape unexplored. In this paper, we explore the field of dependently-typed object-oriented programming by deriving it from first principles using the principle of duality. That is, we do not extend an existing object-oriented formalism with dependent types in an ad-hoc fashion, but instead start from a familiar data-oriented language and derive its dual fragment by the systematic use of defunctionalization and refunctionalization. Our central contribution is a dependently typed calculus which contains two dual language fragments. We provide type- and semantics-preserving transformations between these two language fragments: defunctionalization and refunctionalization. We have implemented this language and these transformations and use this implementation to explain the various ways in which constructions in dependently typed programming can be explained as special instances of the phenomenon of duality.","Mon, 11 Mar 2024 13:33:09 UTC (436 KB)"
"39","A Minority of C++ Objects Account for the Majority of Allocation CPU Time","Eugene Darashkevich, Roman Rusyaev, Roman Korostinskiy, Yegor Bugayenko","Programming Languages (cs.PL)","In C++, an object can be allocated in static memory, on the stack, or on the heap, where the latter is by the order of magnitude more expensive operation, performance wise, than the first two. However, it is not clear how much overall performance loss may be attributed to the use of on-heap objects in C++ applications. This study aims to fill this gap by analyzing object allocation practices in open-source C++ code, investigating the frequency of stack and heap allocations using real-time dynamic analysis with tools such as DynamoRIO and Valgrind. We found out that the majority of objects (97.2%) are allocated on the stack, with only a small portion (2.8%) allocated on the heap. However, when considering the computational cost of each allocation method, we find that heap allocations account for a substantial 85% of the total CPU cycles consumed by object allocations. These findings underscore the importance of optimization of on-heap object allocations, in C++ programming.","Mon, 11 Mar 2024 13:10:58 UTC (81 KB)"
"40","From S-matrix theory to strings: Scattering data and the commitment to non-arbitrariness","Robert van Leeuwen","History and Philosophy of Physics (physics.hist-ph)","The early history of string theory is marked by a shift from strong interaction physics to quantum gravity. The first string models and associated theoretical framework were formulated in the late 1960s and early 1970s in the context of the S-matrix program for the strong interactions. In the mid-1970s, the models were reinterpreted as a potential theory unifying the four fundamental forces. This paper provides a historical analysis of how string theory was developed out of S-matrix physics, aiming to clarify how modern string theory, as a theory detached from experimental data, grew out of an S-matrix program that was strongly dependent upon observable quantities. Surprisingly, the theoretical practice of physicists already turned away from experiment before string theory was recast as a potential unified quantum gravity theory. With the formulation of dual resonance models (the ""hadronic string theory""), physicists were able to determine almost all of the models' parameters on the basis of theoretical reasoning. It was this commitment to ""non-arbitrariness"", i.e., a lack of free parameters in the theory, that initially drove string theorists away from experimental input, and not the practical inaccessibility of experimental data in the context of quantum gravity physics. This is an important observation when assessing the role of experimental data in string theory.","Mon, 11 Mar 2024 13:05:08 UTC (712 KB)"
"41","Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code","Cristina Improta","Cryptography and Security (cs.CR)","AI-based code generators have gained a fundamental role in assisting developers in writing software starting from natural language (NL). However, since these large language models are trained on massive volumes of data collected from unreliable online sources (e.g., GitHub, Hugging Face), AI models become an easy target for data poisoning attacks, in which an attacker corrupts the training data by injecting a small amount of poison into it, i.e., astutely crafted malicious samples. In this position paper, we address the security of AI code generators by identifying a novel data poisoning attack that results in the generation of vulnerable code. Next, we devise an extensive evaluation of how these attacks impact state-of-the-art models for code generation. Lastly, we discuss potential solutions to overcome this threat.","Mon, 11 Mar 2024 12:47:04 UTC (963 KB)"
"42","Aggregated distribution grid flexibilities in subtransmission grid operational management","Neelotpal Majumdar, Lutz Hofmann","Systems and Control (eess.SY)","Aggregated flexibilities or PQ-capabilities (active and reactive power capabilities) are termed in literature as Feasible Operating Regions (FORs). The FORs from underlying active distribution grids can effectively contribute to the operational management at the HV grid level. The HV buses are allocated aggregated FORs from the underlying MV grids, which are inherently nonlinear and non-convex. Therefore, two approaches are proposed in the paper to apply the FOR constraints in the HV grid operational management. First, a mixed integer linear programming (MILP) based optimization approach for alleviating the HV grid constraint violations is proposed, which addresses the non-convexity of the FOR using piecewise segmentation. Furthermore, the MILP method is enhanced to consider the influence of the HV bus voltage on the underlying MV grid flexibilities resulting in a three dimensional PQ(V)-FOR. Second, a convexification approach is proposed, which uses a convex approximation of the non-convex 3D PQ(V)-FOR shape for implementation in a linear optimization method. Results reveal a robust utilization of the distribution flexibilities to maintain grid security and reliability at the HV grid level. Comparisons present increased computation times for the MILP method which are significantly improved using the convexification based approach.","Mon, 11 Mar 2024 11:47:07 UTC (5,613 KB)"
"43","Balanced Substructures in Bicolored Graphs","P. S. Ardra, R. Krithika, Saket Saurabh, Roohani Sharma","Data Structures and Algorithms (cs.DS)","An edge-colored graph is said to be balanced if it has an equal number of edges of each color. Given a graph $G$ whose edges are colored using two colors and a positive integer $k$, the objective in the Edge Balanced Connected Subgraph problem is to determine if $G$ has a balanced connected subgraph containing at least $k$ edges. We first show that this problem is NP-complete and remains so even if the solution is required to be a tree or a path. Then, we focus on the parameterized complexity of Edge Balanced Connected Subgraph and its variants (where the balanced subgraph is required to be a path/tree) with respect to $k$ as the parameter. Towards this, we show that if a graph has a balanced connected subgraph/tree/path of size at least $k$, then it has one of size at least $k$ and at most $f(k)$ where $f$ is a linear function. We use this result combined with dynamic programming algorithms based on color coding and representative sets to show that Edge Balanced Connected Subgraph and its variants are FPT. Further, using polynomial-time reductions to the Multilinear Monomial Detection problem, we give faster randomized FPT algorithms for the problems. In order to describe these reductions, we define a combinatorial object called relaxed-subgraph. We define this object in such a way that balanced connected subgraphs, trees and paths are relaxed-subgraphs with certain properties. This object is defined in the spirit of branching walks known for the Steiner Tree problem and may be of independent interest.","Mon, 11 Mar 2024 10:52:22 UTC (34 KB)"
"44","Study of dark matter scattering off ${}^2$H and ${}^4$He nuclei within chiral effective field theory","E. Filandri, M. Viviani","High Energy Physics - Phenomenology (hep-ph)","We study dark matter, assumed to be composed by weak interacting massive particles (WIMPs), scattering off ${}^2$H and ${}^4$He nuclei. In order to parameterize the WIMP-nucleon interaction the chiral effective field theory approach is used. Considering only interactions invariant under parity, charge conjugation and time reversal, we examine five interaction types: scalar, pseudoscalar, vector, axial and tensor. Scattering amplitudes between two nucleons and a WIMP are determined up to second order of chiral perturbation theory. We apply this program to calculate the interaction rate as function of the WIMP mass and of the magnitude of the WIMP-quark coupling constants. From our study, we conclude that the scalar nuclear response functions result much greater than the others due to theirs large combination of low energy constants. We verify that the leading order contributions are dominant in this low energy processes. We also provide an estimate for the background due to atmospheric neutrinos.","Mon, 11 Mar 2024 10:45:26 UTC (217 KB)"
"45","Ride-pooling Electric Autonomous Mobility-on-Demand: Joint Optimization of Operations and Fleet and Infrastructure Design","Fabio Paparella, Karni Chauhan, Luc Koenders, Theo Hofman, Mauro Salazar","Systems and Control (eess.SY)","This paper presents a modeling and design optimization framework for an Electric Autonomous Mobility-on-Demand system that allows for ride-pooling, i.e., multiple users can be transported at the same time towards a similar direction to decrease vehicle hours traveled by the fleet at the cost of additional waiting time and delays caused by detours. In particular, we first devise a multi-layer time-invariant network flow model that jointly captures the position and state of charge of the vehicles. Second, we frame the time-optimal operational problem of the fleet, including charging and ride-pooling decisions as a mixed-integer linear program, whereby we jointly optimize the placement of the charging infrastructure. Finally, we perform a case-study using Manhattan taxi-data. Our results indicate that jointly optimizing the charging infrastructure placement allows to decrease overall energy consumption of the fleet and vehicle hours traveled by approximately 1% compared to an heuristic placement. Most significantly, ride-pooling can decrease such costs considerably more, and up to 45%. Finally, we investigate the impact of the vehicle choice on the energy consumption of the fleet, comparing a lightweight two-seater with a heavier four-seater, whereby our results show that the former and latter designs are most convenient for low- and high-demand areas, respectively.","Mon, 11 Mar 2024 10:06:38 UTC (12,701 KB)"
"46","Towards Fixed-Point Formats Determination for Faust Programs","Agathe Herrou (GRAME), Florent de Dinechin (INSA Lyon), Stéphane Letz (GRAME), Yann Orlarey, Anastasia Volkova","Programming Languages (cs.PL)","Modern programmable digital signal processing relies on floating-point numbers for their ease of use. Fixed-point number formats have the potential to save resources and improve execution time, but realising this potential burdens the programmer with the need to define each format, at every step of the computation. This article reviews existing methods to automatically determine fixed-point formats, then describes and evaluates the prototype implementation of automatic fixed-point format determination in the Faust compiler.","Mon, 11 Mar 2024 09:06:20 UTC (575 KB)"
"47","Automatic Generation of Python Programs Using Context-Free Grammars","Kamel Yamani, Marwa Naïr, Riyadh Baghdadi","Programming Languages (cs.PL)","In recent years, data has emerged as the new gold, serving as a powerful tool for creating intelligent systems. However, procuring high-quality data remains challenging, especially for code. To address this, we developed TinyPy Generator, a tool that generates random Python programs using a context-free grammar. The generated programs are guaranteed to be correct by construction. Our system uses custom production rules (in the Backus-Naur Form (BNF) format) to recursively generate code. This allows us to generate code with different levels of complexity, ranging from code containing only assignments to more complex code containing conditionals and loops. Our proposed tool enables effortless large-scale Python code generation, beneficial for a wide range of applications. TinyPy Generator is particularly useful in the field of machine learning, where it can generate substantial amounts of Python code for training Python language models. Additionally, researchers who are studying programming languages can utilize this tool to create datasets for their experiments, which can help validate the robustness of code interpreters or compilers. Unlike existing research, we have open-sourced our implementation. This allows customization according to user needs and extends potential usage to other languages.","Mon, 11 Mar 2024 08:25:52 UTC (592 KB)"
"48","An Efficient Solution to the 2D Visibility Problem in Cartesian Grid Maps and its Application in Heuristic Path Planning","Ibrahim Ibrahim, Joris Gillis, Wilm Decré, Jan Swevers","Computational Geometry (cs.CG)","This paper introduces a novel, lightweight method to solve the visibility problem for 2D grids. The proposed method evaluates the existence of lines-of-sight from a source point to all other grid cells in a single pass with no preprocessing and independently of the number and shape of obstacles. It has a compute and memory complexity of $\mathcal{O}(n)$, where $n = n_{x}\times{} n_{y}$ is the size of the grid, and requires at most ten arithmetic operations per grid cell. In the proposed approach, we use a linear first-order hyperbolic partial differential equation to transport the visibility quantity in all directions. In order to accomplish that, we use an entropy-satisfying upwind scheme that converges to the true visibility polygon as the step size goes to zero. This dynamic-programming approach allows the evaluation of visibility for an entire grid orders of magnitude faster than typical ray-casting algorithms. We provide a practical application of our proposed algorithm by posing the visibility quantity as a heuristic and implementing a deterministic, local-minima-free path planner, setting apart the proposed planner from traditional methods. Lastly, we provide necessary algorithms and an open-source implementation of the proposed methods.","Mon, 11 Mar 2024 08:07:11 UTC (5,191 KB)"
"49","AGAThA: Fast and Efficient GPU Acceleration of Guided Sequence Alignment for Long Read Mapping","Seongyeon Park, Junguk Hong, Jaeyong Song, Hajin Kim, Youngsok Kim, Jinho Lee","Distributed, Parallel, and Cluster Computing (cs.DC)","With the advance in genome sequencing technology, the lengths of deoxyribonucleic acid (DNA) sequencing results are rapidly increasing at lower prices than ever. However, the longer lengths come at the cost of a heavy computational burden on aligning them. For example, aligning sequences to a human reference genome can take tens or even hundreds of hours. The current de facto standard approach for alignment is based on the guided dynamic programming method. Although this takes a long time and could potentially benefit from high-throughput graphic processing units (GPUs), the existing GPU-accelerated approaches often compromise the algorithm's structure, due to the GPU-unfriendly nature of the computational pattern. Unfortunately, such compromise in the algorithm is not tolerable in the field, because sequence alignment is a part of complicated bioinformatics analysis pipelines. In such circumstances, we propose AGAThA, an exact and efficient GPU-based acceleration of guided sequence alignment. We diagnose and address the problems of the algorithm being unfriendly to GPUs, which comprises strided/redundant memory accesses and workload imbalances that are difficult to predict. According to the experiments on modern GPUs, AGAThA achieves 18.8$\times$ speedup against the CPU-based baseline, 9.6$\times$ against the best GPU-based baseline, and 3.6$\times$ against GPU-based algorithms with different heuristics.","Mon, 11 Mar 2024 07:34:59 UTC (1,142 KB)"
"50","An Alternative to Stride-Based RNG for Monte Carlo Transport","Braxton S.Cuneo, Ilham Variansyah","Computational Physics (physics.comp-ph)","The techniques used to generate pseudo-random numbers for Monte Carlo (MC) applications bear many implications on the quality and speed of that programs work. As a random number generator (RNG) slows, the production of random numbers begins to dominate runtime. As RNG output grows in correlation, the final product becomes less reliable. These difficulties are further compounded by the need for reproducibility and parallelism. For reproducibility, the numbers generated to determine any outcome must be the same each time a simulation is run. However, the concurrency that comes with most parallelism introduces race conditions. To have both reproducibility and concurrency, separate RNG states must be tracked for each independently schedulable unit of simulation, forming independent random number streams. We propose an alternative to the stride-based parallel LCG seeding approach that scales more practically with increased concurrency and workload by generating seeds through hashing and allowing for repeated outputs. Data gathered from normality tests of tally results from simple MC transport benchmark calculations indicates that the proposed hash-based RNG does not significantly affect the tally result normality property as compared to the conventional stride-based RNG.","Mon, 11 Mar 2024 01:19:42 UTC (3,411 KB)"
"51","Quantum physics, digital computers, and life from a holistic perspective","George F R Ellis","Quantum Physics (quant-ph)","Quantum physics is a linear theory, so it is somewhat puzzling that it can underlie very complex systems such as digital computers and life. This paper investigates how this is possible. Physically, such complex systems are necessarily modular hierarchical structures, with a number of key features. Firstly, they cannot be described by a single wave function: only local wave functions can exist, rather than a single wave function for a living cell, a cat, or a brain. Secondly, the quantum to classical transition is characterised by contextual wave-function collapse shaped by macroscopic elements that can be described classically. Thirdly, downward causation occurs in the physical hierarchy in two key ways: by the downward influence of time dependent constraints, and by creation, modification, or deletion of lower level elements. Fourthly, there are also logical modular hierarchical structures supported by the physical ones, such as algorithms and computer programs, They are able to support arbitrary logical operations, which can influence physical outcomes as in computer aided design and 3-d printing. Finally, complex systems are necessarily open systems, with heat baths playing a key role in their dynamics and providing local arrows of time that agree with the cosmological direction of time that is established by the evolution of the universe.","Sun, 10 Mar 2024 20:46:37 UTC (1,637 KB)"
"52","mpbn: a simple tool for efficient edition and analysis of elementary properties of Boolean networks","Van-Giang Trinh, Belaid Benhamou, Loïc Paulevé","Logic in Computer Science (cs.LO)","The tool mpbn offers a Python programming interface for an easy interactive editing of Boolean networks and the efficient computation of elementary properties of their dynamics, including fixed points, trap spaces, and reachability properties under the Most Permissive update mode. Relying on Answer-Set Programming logical framework, we show that mpbn is scalable to models with several thousands of nodes and is one of the best-performing tool for computing minimal and maximal trap spaces of Boolean networks, a key feature for understanding and controling their stable behaviors. The tool is available at this https URL.","Sun, 10 Mar 2024 16:40:35 UTC (214 KB)"
"53","LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard's Capacity to Handle Object-Oriented Programming Assignments","Bruno Pereira Cipriano, Pedro Alves","Software Engineering (cs.SE)","Large Language Models (LLMs) have emerged as promising tools to assist students while solving programming assignments. However, object-oriented programming (OOP), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. Contrary to introductory programming exercises, there exists a research gap with regard to the behavior of LLMs in OOP contexts. In this study, we experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve real-world OOP exercises used in educational settings, subsequently validating their solutions using an Automatic Assessment Tool (AAT). The findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of OOP. GPT-4 stood out as the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing LLMs with AATs in pedagogical settings. In conclusion, while GPT-4 showcases promise, the deployment of these models in OOP education still mandates supervision.","Sun, 10 Mar 2024 16:40:05 UTC (579 KB)"
"54","Robust Predictive Motion Planning by Learning Obstacle Uncertainty","Jian Zhou, Yulong Gao, Ola Johansson, Björn Olofsson, Erik Frisk","Robotics (cs.RO)","Safe motion planning for robotic systems in dynamic environments is nontrivial in the presence of uncertain obstacles, where estimation of obstacle uncertainties is crucial in predicting future motions of dynamic obstacles. The worst-case characterization gives a conservative uncertainty prediction and may result in infeasible motion planning for the ego robotic system. In this paper, an efficient, robust, and safe motion-planing algorithm is developed by learning the obstacle uncertainties online. More specifically, the unknown yet intended control set of obstacles is efficiently computed by solving a linear programming problem. The learned control set is used to compute forward reachable sets of obstacles that are less conservative than the worst-case prediction. Based on the forward prediction, a robust model predictive controller is designed to compute a safe reference trajectory for the ego robotic system that remains outside the reachable sets of obstacles over the prediction horizon. The method is applied to a car-like mobile robot in both simulations and hardware experiments to demonstrate its effectiveness.","Sun, 10 Mar 2024 13:59:18 UTC (3,127 KB)"
"55","Direct Shooting Method for Numerical Optimal Control: A Modified Transcription Approach","Jiawei Tang, Yuxing Zhong, Pengyu Wang, Xingzhou Chen, Shuang Wu, Ling Shi","Systems and Control (eess.SY)","Direct shooting is an efficient method to solve numerical optimal control. It utilizes the Runge-Kutta scheme to discretize a continuous-time optimal control problem making the problem solvable by nonlinear programming solvers. However, conventional direct shooting raises a contradictory dynamics issue when using an augmented state to handle {high-order} systems. This paper fills the research gap by considering the direct shooting method for {high-order} systems. We derive the modified Euler and Runge-Kutta-4 methods to transcribe the system dynamics constraint directly. Additionally, we provide the global error upper bounds of our proposed methods. A set of benchmark optimal control problems shows that our methods provide more accurate solutions than existing approaches.","Sun, 10 Mar 2024 10:36:39 UTC (1,099 KB)"
"56","Are LLMs ready for Visualization?","Pere-Pau Vázquez","Human-Computer Interaction (cs.HC)","Generative models have received a lot of attention in many areas of academia and the industry. Their capabilities span many areas, from the invention of images given a prompt to the generation of concrete code to solve a certain programming issue. These two paradigmatic cases fall within two distinct categories of requirements, ranging from ""creativity"" to ""precision"", as characterized by Bing Chat, which employs ChatGPT-4 as its backbone. Visualization practitioners and researchers have wondered to what end one of such systems could accomplish our work in a more efficient way. Several works in the literature have utilized them for the creation of visualizations. And some tools such as Lida, incorporate them as part of their pipeline. Nevertheless, to the authors' knowledge, no systematic approach for testing their capabilities has been published, which includes both extensive and in-depth evaluation. Our goal is to fill that gap with a systematic approach that analyzes three elements: whether Large Language Models are capable of correctly generating a large variety of charts, what libraries they can deal with effectively, and how far we can go to configure individual charts. To achieve this objective, we initially selected a diverse set of charts, which are commonly utilized in data visualization. We then developed a set of generic prompts that could be used to generate them, and analyzed the performance of different LLMs and libraries. The results include both the set of prompts and the data sources, as well as an analysis of the performance with different configurations.","Sun, 10 Mar 2024 10:09:34 UTC (2,689 KB)"
"57","Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills","Paul Denny, David H. Smith IV, Max Fowler, James Prather, Brett A. Becker, Juho Leinonen","Human-Computer Interaction (cs.HC)","Reading, understanding and explaining code have traditionally been important skills for novices learning programming. As large language models (LLMs) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. Brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an LLM. Thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with LLMs. One effective way to develop and assess code comprehension ability is with ``Explain in plain English'' (EiPE) questions, where students succinctly explain the purpose of a fragment of code. However, grading EiPE questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. In this paper, we explore a natural synergy between EiPE questions and code-generating LLMs to overcome this limitation. We propose using an LLM to generate code based on students' responses to EiPE questions -- not only enabling EiPE responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. We investigate this idea in an introductory programming course and report student success in creating effective prompts for solving EiPE questions. We also examine student perceptions of this activity and how it influences their views on the use of LLMs for aiding and assessing learning.","Sun, 10 Mar 2024 00:23:08 UTC (192 KB)"
"58","A Tool for Automated Reasoning About Traces Based on Configurable Formal Semantics","Ferhat Erata, Arda Goknil, Bedir Tekinerdogan, Geylani Kardas","Software Engineering (cs.SE)","We present Tarski, a tool for specifying configurable trace semantics to facilitate automated reasoning about traces. Software development projects require that various types of traces be modeled between and within development artifacts. For any given artifact (e.g., requirements, architecture models and source code), Tarski allows the user to specify new trace types and their configurable semantics, while, using the semantics, it automatically infers new traces based on existing traces provided by the user, and checks the consistency of traces. It has been evaluated on three industrial case studies in the automotive domain (this https URL).","Sat, 9 Mar 2024 21:23:40 UTC (2,860 KB)"
"59","Training physical matter to matter","Heinrich M. Jaeger, Arvind Murugan, Sidney R. Nagel","Soft Condensed Matter (cond-mat.soft)","Biological systems offer a great many examples of how sophisticated, highly adapted behavior can emerge from training. Here we discuss how training might be used to impart similarly adaptive properties in physical matter. As a special form of materials processing, training differs in important ways from standard approaches of obtaining sought after material properties. In particular, rather than designing or programming the local configurations and interactions of constituents, training uses externally applied stimuli to evolve material properties. This makes it possible to obtain different functionalities from the same starting material (pluripotency). Furthermore, training evolves a material in-situ or under conditions similar to those during the intended use; thus, material performance can improve rather than degrade over time. We discuss requirements for trainability, outline recently developed training strategies for creating soft materials with multiple, targeted and adaptable functionalities, and provide examples where the concept of training has been applied to materials on length scales from the molecular to the macroscopic.","Sat, 9 Mar 2024 19:17:11 UTC (258 KB)"
"60","Harmonic Balance for Differential Constitutive Models under Oscillatory Shear","Shivangi Mittal, Yogesh M. Joshi, Sachin Shanbhag","Soft Condensed Matter (cond-mat.soft)","Harmonic balance (HB) is a popular Fourier-Galerkin method used in the analysis of nonlinear vibration problems where dynamical systems are subjected to periodic forcing. We adapt HB to find the periodic steady-state response of nonlinear differential constitutive models subjected to large amplitude oscillatory shear flow. By incorporating the alternating-frequency-time scheme into HB, we develop a computer program called FLASH (acronym for Fast Large Amplitude Simulation using Harmonic balance), which makes it convenient to apply HB to any differential constitutive model. We validate FLASH by considering two representative constitutive models, viz., the exponential Phan-Thien Tanner model and a nonlinear temporary network model. In terms of accuracy and speed, FLASH outperforms the conventional approach of solving initial value problems by numerical integration via time-stepping methods often by several orders of magnitude. We discuss how FLASH can be conveniently extended for other nonlinear constitutive models, which opens up potential applications in model calibration and selection, and stability analysis.","Sat, 9 Mar 2024 17:35:34 UTC (515 KB)"
"61","Blockchain-Enhanced Offloading in Mobile Edge Computing: A Systematic Review and Survey of Current Trends and Future Directions","Komeil Moghaddasi, Shakiba Rajabi","Distributed, Parallel, and Cluster Computing (cs.DC)","With the rapid growth of Internet of Things (IoT) applications, there's a big demand for more processing power and resources in devices. Mobile Edge Computing (MEC) looks promising for enhancing performance and reducing costs by offloading the computing work of IoT to MEC servers. However, the current methods for offloading have issues with privacy and security during the transfer of data and programs. To tackle this, recent developments have introduced secure offloading methods using Blockchain technology, which helps to make MEC more secure by building trust between nodes, improving how edges are authenticated and accessed, and stopping unauthorized access to devices. This paper reviews these Blockchain-based offloading methods for different MEC settings. It starts by explaining the key ideas in offloading and Blockchain, then it sorts the Blockchain-based offloading methods by the algorithms they use. It also compares the offloading methods in each group and ends with a discussion and comparison of the different techniques, tools, and metrics used in these methods.","Sat, 9 Mar 2024 17:04:30 UTC (4,897 KB)"
"62","A Novel Refactoring and Semantic Aware Abstract Syntax Tree Differencing Tool and a Benchmark for Evaluating the Accuracy of Diff Tools","Pouria Alikhanifard, Nikolaos Tsantalis","Software Engineering (cs.SE)","Software undergoes constant changes to support new requirements, address bugs, enhance performance, and ensure maintainability. Thus, developers spend a great portion of their workday trying to understand and review the code changes of their teammates. Abstract Syntax Tree (AST) diff tools were developed to overcome the limitations of line-based diff tools, which are used by the majority of developers. Despite the notable improvements brought by AST diff tools in understanding complex changes, they still suffer from serious limitations, such as (1) lacking multi-mapping support, (2) matching semantically incompatible AST nodes, (3) ignoring language clues to guide the matching process, (4) lacking refactoring awareness, and (5) lacking commit-level diff support. We propose a novel AST diff tool based on RefactoringMiner that resolves all aforementioned limitations. First, we improved RefactoringMiner to increase its statement mapping accuracy, and then we developed an algorithm that generates AST diff for a given commit or pull request based on the refactoring instances and pairs of matched program element declarations provided by RefactoringMiner. To evaluate the accuracy of our tool and compare it with the state-of-the-art tools, we created the first benchmark of AST node mappings, including 800 bug-fixing commits and 188 refactoring commits. Our evaluation showed that our tool achieved a considerably higher precision and recall, especially for refactoring commits, with an execution time that is comparable with that of the faster tools.","Sat, 9 Mar 2024 15:32:41 UTC (1,345 KB)"
"63","Deep learning for multi-label classification of coral conditions in the Indo-Pacific via underwater photogrammetry","Xinlei Shao, Hongruixuan Chen, Kirsty Magson, Jiaqi Wang, Jian Song, Jundong Chen, Jun Sasaki","Computer Vision and Pattern Recognition (cs.CV)","Since coral reef ecosystems face threats from human activities and climate change, coral conservation programs are implemented worldwide. Monitoring coral health provides references for guiding conservation activities. However, current labor-intensive methods result in a backlog of unsorted images, highlighting the need for automated classification. Few studies have simultaneously utilized accurate annotations along with updated algorithms and datasets. This study aimed to create a dataset representing common coral conditions and associated stressors in the Indo-Pacific. Concurrently, it assessed existing classification algorithms and proposed a new multi-label method for automatically detecting coral conditions and extracting ecological information. A dataset containing over 20,000 high-resolution coral images of different health conditions and stressors was constructed based on the field survey. Seven representative deep learning architectures were tested on this dataset, and their performance was quantitatively evaluated using the F1 metric and the match ratio. Based on this evaluation, a new method utilizing the ensemble learning approach was proposed. The proposed method accurately classified coral conditions as healthy, compromised, dead, and rubble; it also identified corresponding stressors, including competition, disease, predation, and physical issues. This method can help develop the coral image archive, guide conservation activities, and provide references for decision-making for reef managers and conservationists. The proposed ensemble learning approach outperforms others on the dataset, showing State-Of-The-Art (SOTA) performance. Future research should improve its generalizability and accuracy to support global coral conservation efforts.","Sat, 9 Mar 2024 14:42:16 UTC (1,431 KB)[v2] Tue, 12 Mar 2024 14:15:50 UTC (1,403 KB)"
"64","XFLUIDS: A SYCL-based unified cross-architecture heterogeneous simulation solver for compressible reacting flows","Jinlong Li, Shucheng Pan","Computational Physics (physics.comp-ph)","We present a cross-architecture large-scale heterogeneous Navier-Stokes equation solver, XFLUIDS, for compressible reacting multicomponent flows in the three-dimensional structured mesh crossing different GPU platforms. The multi-component reacting flows are ubiquitous in many situations, with high-order FDM schemes used and calculation of reaction source integration, their numerical simulations are time-consuming to achieve high accuracy and capture the underlying multiscale features, and GPU-based acceleration methods are extremely helpful. At the same time, various accelerators have been released in the market, leading to a challenge of applications' adaptability. Based on an implementation of a heterogeneous programming standard, SYCL, it is now possible to develop heterogeneous parallel acceleration applications directly targeting multi-core CPUs, FPGAs, and GPUs from Nvidia, AMD, as well as Intel without translating any source code, demonstrating its high portability. MPI library is used to extend the solver to multi-GPU accelerated computing, with the GPU-ENABLED MPI technology supporting the direct data transmission between GPUs to reduce communication latency, and the weak-scaling and strong-scaling efficiency tested. This solver has been validated by several benchmark cases, including shock tube problems, diffusion problems, nitrogen dissociation problems, shock-bubble interaction problems, etc.","Sat, 9 Mar 2024 13:28:42 UTC (2,703 KB)"
"65","Towards Multiphase Clocking in Single-Flux Quantum Systems","Rassul Bairamkulov, Giovanni De Micheli","Emerging Technologies (cs.ET)","Rapid single-flux quantum (RSFQ) is one of the most advanced superconductive electronics technologies. SFQ systems operate at tens of gigahertz with up to three orders of magnitude smaller power as compared to CMOS. In conventional SFQ systems, most gates require clock signal. Each gate should have the fanins with equal logic depth, necessitating insertion of path-balancing (PB) DFFs, incurring prohibitive area penalty. Multiphase clocking is the effective method for reducing the path-balancing overhead at the cost of reduced throughput. However, existing tools are not directly applicable for technology mapping of multiphase systems. To overcome this limitation, in this work, we propose a technology mapping tool for multiphase systems. Our contribution is threefold. First, we formulate a phase assignment as a Constraint Programming with Satisfiability (CP-SAT) problem, to determine the phase of each element within the network. Second, we formulate the path balancing problem as a CP-SAT to optimize the number of DFFs within an asynchronous datapath. Finally, we integrate these methods into a technology mapping flow to convert a logic network into a multiphase SFQ circuit. In our case studies, by using seven phases, the size of the circuit (expressed as the number of Josephson junctions) is reduced, on average, by 59.94 % as compared to the dual (fast-slow) clocking method, while outperforming the state-of-the-art single-phase SFQ mapping tools.","Sat, 9 Mar 2024 11:34:25 UTC (4,355 KB)"
"66","DeepVM: Integrating Spot and On-Demand VMs for Cost-Efficient Deep Learning Clusters in the Cloud","Yoochan Kim, Kihyun Kim, Yonghyeon Cho, Jinwoo Kim, Awais Khan, Ki-Dong Kang, Baik-Song An, Myung-Hoon Cha, Hong-Yeon Kim, Youngjae Kim","Distributed, Parallel, and Cluster Computing (cs.DC)","Distributed Deep Learning (DDL), as a paradigm, dictates the use of GPU-based clusters as the optimal infrastructure for training large-scale Deep Neural Networks (DNNs). However, the high cost of such resources makes them inaccessible to many users. Public cloud services, particularly Spot Virtual Machines (VMs), offer a cost-effective alternative, but their unpredictable availability poses a significant challenge to the crucial checkpointing process in DDL. To address this, we introduce DeepVM, a novel solution that recommends cost-effective cluster configurations by intelligently balancing the use of Spot and On-Demand VMs. DeepVM leverages a four-stage process that analyzes instance performance using the FLOPP (FLoating-point Operations Per Price) metric, performs architecture-level analysis with linear programming, and identifies the optimal configuration for the user-specific needs. Extensive simulations and real-world deployments in the AWS environment demonstrate that DeepVM consistently outperforms other policies, reducing training costs and overall makespan. By enabling cost-effective checkpointing with Spot VMs, DeepVM opens up DDL to a wider range of users and facilitates a more efficient training of complex DNNs.","Sat, 9 Mar 2024 10:17:14 UTC (747 KB)"
"67","UniSparse: An Intermediate Language for General Sparse Format Customization","Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, Zhiru Zhang","Computation and Language (cs.CL)","The ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. These formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. Unlike the existing attribute-based frameworks, UniSparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. As a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory (PIM) device.","Sat, 9 Mar 2024 05:38:45 UTC (1,733 KB)"
"68","Scalable $k$-clique Densest Subgraph Search","Xiaowei Ye, Miao Qiao, Rong-Hua Li, Qi Zhang, Guoren Wang","Data Structures and Algorithms (cs.DS)","In this paper, we present a collection of novel and scalable algorithms designed to tackle the challenges inherent in the $k$-clique densest subgraph problem (\kcdsp) within network analysis. We propose \psctl, a novel algorithm based on the Frank-Wolfe approach for addressing \kcdsp, effectively solving a distinct convex programming problem. \textcolor{black}{\psctl is able to approximate \kcdsp with near optimal guarantees.} The notable advantage of \psctl lies in its time complexity, which is independent of the count of $k$-cliques, resulting in remarkable efficiency in practical applications. Additionally, we present \spath, a sampling-based algorithm with the capability to handle networks on an unprecedented scale, reaching up to $1.8\times 10^9$ edges. By leveraging the \ccpath algorithm as a uniform $k$-clique sampler, \spath ensures the efficient processing of large-scale network data, accompanied by a detailed analysis of accuracy guarantees. Together, these contributions represent a significant advancement in the field of $k$-clique densest subgraph discovery. In experimental evaluations, our algorithms demonstrate orders of magnitude faster performance compared to the current state-of-the-art solutions.","Sat, 9 Mar 2024 03:08:29 UTC (35,692 KB)"
"69","Efficient Public Health Intervention Planning Using Decomposition-Based Decision-Focused Learning","Sanket Shah, Arun Suggala, Milind Tambe, Aparna Taneja","Artificial Intelligence (cs.AI)","The declining participation of beneficiaries over time is a key concern in public health programs. A popular strategy for improving retention is to have health workers `intervene' on beneficiaries at risk of dropping out. However, the availability and time of these health workers are limited resources. As a result, there has been a line of research on optimizing these limited intervention resources using Restless Multi-Armed Bandits (RMABs). The key technical barrier to using this framework in practice lies in the need to estimate the beneficiaries' RMAB parameters from historical data. Recent research has shown that Decision-Focused Learning (DFL), which focuses on maximizing the beneficiaries' adherence rather than predictive accuracy, improves the performance of intervention targeting using RMABs. Unfortunately, these gains come at a high computational cost because of the need to solve and evaluate the RMAB in each DFL training step. In this paper, we provide a principled way to exploit the structure of RMABs to speed up intervention planning by cleverly decoupling the planning for different beneficiaries. We use real-world data from an Indian NGO, ARMMAN, to show that our approach is up to two orders of magnitude faster than the state-of-the-art approach while also yielding superior model performance. This would enable the NGO to scale up deployments using DFL to potentially millions of mothers, ultimately advancing progress toward UNSDG 3.1.","Fri, 8 Mar 2024 21:31:00 UTC (7,434 KB)"
